{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import autogen\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Annotated, Tuple, Union\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 42,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "class HumanReadbleResponseAgent(autogen.AssistantAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         llm_config=False, \n",
    "                         system_message=\"\"\"You are a logger that should reply with a summary of the conversation so far. \n",
    "Use the previous messages when creating the summary. \n",
    "\n",
    "Your reply should follow this format: \n",
    "\n",
    "[Summary of what has happened so far, aimed at a human end user]\n",
    "\n",
    "[TERMINATE/CONTINUE]\n",
    "\n",
    "The end of your response should be either \"TERMINATE\" or \"CONTINUE\".\n",
    "Reply \"TERMINATE\" if the task has been solved at full satisfaction. \n",
    "Otherwise, reply \"CONTINUE\", or the reason why the task is not solved yet.\n",
    "Make sure \"TERMINATE\" and \"CONTINUE\" are in capital letters.  \n",
    "\"\"\", \n",
    "                         **kwargs)\n",
    "        self.register_reply([autogen.Agent, None], HumanReadbleResponseAgent.produce_response)\n",
    "    \n",
    "    def produce_response(\n",
    "        self,\n",
    "        messages: List[Dict] = [],\n",
    "        sender=None,\n",
    "        config=None,\n",
    "    ) -> Tuple[bool, Union[str, Dict, None]]:\n",
    "        client = OpenAI()\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        response = ''\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                response += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\")\n",
    "        return True, response\n",
    "\n",
    "human_readble_assistant = HumanReadbleResponseAgent(name='human_readble_assistant')\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  \n",
    "    },\n",
    ")\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@assistant.register_for_llm(description=\"Calculator for doing division.\")\n",
    "def division_calculator(\n",
    "    dividend: Annotated[int, \"The number that is being divided\"],\n",
    "    divisor: Annotated[int, \"The number that the dividend being divided by\"]\n",
    ") -> str: # return type must be either str, BaseModel or serializable by json.dumps()\n",
    "  if divisor == 0:\n",
    "    return 'Division by 0 is not allowed!'\n",
    "  return f'{dividend / divisor}'\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, assistant, human_readble_assistant], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, system_message='Always use \"human_readble_assistant\" at the end of the conversation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "What is 7 divided by 3?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: division_calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"dividend\": 7,\n",
      "  \"divisor\": 3\n",
      "}\n",
      "\u001b[32m********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION division_calculator...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"division_calculator\" *****\u001b[0m\n",
      "2.3333333333333335\n",
      "\u001b[32m****************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "The result of 7 divided by 3 is approximately 2.33. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Ok, if you have any other questions, feel free to ask!\u001b[33mhuman_readble_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Ok, if you have any other questions, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# user_proxy.initiate_chat(assistant, message='What is 7 divided by 3?')\n",
    "user_proxy.initiate_chat(manager, message='What is 7 divided by 3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a logger that should reply with a summary of the conversation so far. \n",
      "Use the previous messages when creating the summary. \n",
      "\n",
      "Your reply should follow this format: \n",
      "\n",
      "[Summary of what has happened so far, aimed at a human end user]\n",
      "\n",
      "[TERMINATE/CONTINUE]\n",
      "\n",
      "The end of your response should be either \"TERMINATE\" or \"CONTINUE\".\n",
      "Reply \"TERMINATE\" if the task has been solved at full satisfaction. \n",
      "Otherwise, reply \"CONTINUE\", or the reason why the task is not solved yet.\n",
      "Make sure \"TERMINATE\" and \"CONTINUE\" are in capital letters.  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function __main__.HumanReadbleResponseAgent.produce_response(self, messages: List[Dict] = [], sender=None, config=None) -> Tuple[bool, Union[str, Dict, NoneType]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.a_check_termination_and_human_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[str]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.check_termination_and_human_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[str]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.a_generate_function_call_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[Dict]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.generate_function_call_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[Dict]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.generate_code_execution_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Union[Dict, Literal[False], NoneType] = None)>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.a_generate_oai_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, NoneType]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.generate_oai_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[autogen.oai.client.OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, NoneType]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.chat_messages\n",
    "print(human_readble_assistant.system_message)\n",
    "human_readble_assistant._reply_func_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
