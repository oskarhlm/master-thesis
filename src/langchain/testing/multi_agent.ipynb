{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from enum import Enum\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict, Union, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "import operator\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "import os\n",
    "\n",
    "class Worker(Enum):\n",
    "    CODE = 'code_worker'\n",
    "    SUMMARY = 'summary_worker'\n",
    "\n",
    "    @classmethod\n",
    "    def get_description(cls, worker: 'Worker'):    \n",
    "        return {\n",
    "            Worker.CODE.value: 'A coding worker/agent that can produce and execute Python code',\n",
    "            Worker.SUMMARY.value: 'A summary writing worker/agent that can produce summaries of human-AI conversations'\n",
    "        }.get(worker.value)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    initial_query: str\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: Worker\n",
    "\n",
    "\n",
    "def create_agent_supervisor(workers: Sequence[Worker]):\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        \" following workers:\\n{workers}\\n\\n Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    options = [\"FINISH\"] + [m.value for m in workers]\n",
    "\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    bullet_point_list = \"\\n\".join(f\"{i+1}) {worker.value} - {Worker.get_description(worker)}\"\n",
    "                                  for i, worker in enumerate(workers))\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                ( \n",
    "                    \"Given the conversation above, who should act next, if any?\"\n",
    "                    ' Return \"FINISH\" if the initial human query (\"{initial_query}\") has been answered?\\n\\n'\n",
    "                    \"Select one of: {options}\\n\\n\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ).partial(options=options, workers=bullet_point_list)\n",
    "\n",
    "    llm = ChatOpenAI(model=os.getenv('GPT4_MODEL_NAME'), streaming=True)\n",
    "\n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: Sequence[BaseTool], system_prompt: str):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(tools) == 0:\n",
    "        return (\n",
    "            ChatPromptTemplate.from_template(system_prompt)\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    "\n",
    "\n",
    "async def agent_node(state: AgentState, agent, name):\n",
    "    result = await agent.ainvoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result['output'], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "import os\n",
    "\n",
    "supervisor_chain = create_agent_supervisor([e for e in Worker])\n",
    "\n",
    "llm = ChatOpenAI(model=os.getenv('GPT3_MODEL_NAME'), streaming=True)\n",
    "\n",
    "summary_agent = create_agent(llm, [], \"You are a summary agent. Write a summary of the conversation so far: {messages}\")\n",
    "summary_node = functools.partial(\n",
    "    agent_node, agent=summary_agent, name=\"Summarizer\")\n",
    "\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [PythonREPLTool()],\n",
    "    \"You are a coding agent.\",\n",
    ")\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "SUPERVISOR = 'supervisor'\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(Worker.SUMMARY.value, summary_node)\n",
    "workflow.add_node(Worker.CODE.value, code_node)\n",
    "workflow.add_node(SUPERVISOR, supervisor_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in Worker:\n",
    "    workflow.add_edge(worker.value, SUPERVISOR)\n",
    "\n",
    "conditional_map = {k.value: k.value for k in Worker}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\n",
    "    SUPERVISOR, lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "workflow.set_entry_point(SUPERVISOR)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [HumanMessage(content=\"What is 576 * 5 / 13?\")]\n",
    "# async for event in app.astream_events(\n",
    "#     {\n",
    "#         'initial_query': messages[0].content,\n",
    "#         \"messages\": messages\n",
    "#     },\n",
    "#     {\"recursion_limit\": 100},\n",
    "#     version=\"v1\"\n",
    "# ):\n",
    "#     kind = event[\"event\"]\n",
    "#     print(event)\n",
    "    # if kind == \"on_chat_model_stream\":\n",
    "    #     content = event[\"data\"][\"chunk\"].content\n",
    "    #     if content:\n",
    "    #         # Empty content in the context of OpenAI means\n",
    "    #         # that the model is asking for a tool to be invoked.\n",
    "    #         # So we only print non-empty content\n",
    "    #         print(content, end=\"|\")\n",
    "    # elif kind == \"on_tool_start\":\n",
    "    #     print(\"--\")\n",
    "    #     print(\n",
    "    #         f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "    #     )\n",
    "    # elif kind == \"on_tool_end\":\n",
    "    #     print(f\"Done tool: {event['name']}\")\n",
    "    #     print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "    #     print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'code_worker'}}\n",
      "----\n",
      "{'code_worker': {'messages': [HumanMessage(content='Here are the first 30 numbers of the sequence \\\\( a^3 - \\\\frac{1}{a} \\\\):\\n\\n1. 0\\n2. 7\\n3. 26\\n4. 63\\n5. 124\\n6. 215\\n7. 342\\n8. 511\\n9. 728\\n10. 999\\n11. 1330\\n12. 1727\\n13. 2206\\n14. 2773\\n15. 3434\\n16. 4195\\n17. 5062\\n18. 6041\\n19. 7138\\n20. 8360\\n21. 9713\\n22. 11204\\n23. 12839\\n24. 14624\\n25. 16565\\n26. 18668\\n27. 20939\\n28. 23384\\n29. 26009\\n30. 28820\\n\\nThese are the values obtained by substituting values from 1 to 30 into the expression \\\\( a^3 - \\\\frac{1}{a} \\\\).', name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "initial_msg = \"What are the first 30 numbers of the sequence a^3 - 1/a?\"\n",
    "\n",
    "async for s in app.astream(\n",
    "    {\n",
    "        'initial_query': initial_msg,\n",
    "        \"messages\": [HumanMessage(\n",
    "        content=initial_msg)]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
