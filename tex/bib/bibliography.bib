@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2024-03-21},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\DLCZU3RT\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;C\:\\Users\\oskar\\Zotero\\storage\\H729V98D\\1409.html}
}

@misc{chanChatEvalBetterLLMbased2023,
  title = {{{ChatEval}}: {{Towards Better LLM-based Evaluators}} through {{Multi-Agent Debate}}},
  shorttitle = {{{ChatEval}}},
  author = {Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07201},
  eprint = {2308.07201},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07201},
  url = {http://arxiv.org/abs/2308.07201},
  urldate = {2024-03-21},
  abstract = {Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\X7UJH4ZY\\Chan et al. - 2023 - ChatEval Towards Better LLM-based Evaluators thro.pdf;C\:\\Users\\oskar\\Zotero\\storage\\6I5RF2TI\\2308.html}
}

@misc{crunchydataCrunchyDataPg_featureserv2024,
  title = {{{CrunchyData}}/Pg\_featureserv},
  author = {{CrunchyData}},
  year = {2024},
  month = apr,
  url = {https://github.com/CrunchyData/pg_featureserv},
  urldate = {2024-04-19},
  abstract = {Lightweight RESTful Geospatial Feature Server for PostGIS in Go},
  copyright = {Apache-2.0},
  howpublished = {Crunchy Data}
}

@misc{eletiFunctionCallingOther2023,
  title = {Function Calling and Other {{API}} Updates},
  author = {Eleti, Atty and Harris, Jeff and Kilpatrick, Logan},
  year = {2023},
  month = jun,
  url = {https://openai.com/blog/function-calling-and-other-api-updates},
  urldate = {2024-03-10},
  abstract = {We're announcing updates including more steerable API models, function calling capabilities, longer context, and lower prices.},
  langid = {american},
  file = {C:\Users\oskar\Zotero\storage\6X3BUD24\function-calling-and-other-api-updates.html}
}

@misc{guMambaLinearTimeSequence2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2024-03-15},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\FHDRYWMI\\Gu and Dao - 2023 - Mamba Linear-Time Sequence Modeling with Selectiv.pdf;C\:\\Users\\oskar\\Zotero\\storage\\DDJWCSFP\\2312.html}
}

@phdthesis{holmLLMsDeathGIS2023,
  type = {Specialization Project},
  title = {{{LLMs}} - {{The Death}} of {{GIS Analysis}}?},
  author = {Holm, Oskar},
  year = {2023},
  month = dec,
  address = {Trondheim},
  url = {https://github.com/oskarhlm/prosjektoppgave/blob/main/tex/auxiliary/main.pdf},
  abstract = {The emergence of powerful LLMs with remarkable reasoning and coding capabilities - like GPT-4, the latest additions to the GPT series - enables automation of a wide range of tasks. ChatGPT's Code Interpreter is able to generate, execute, and review its own code, making development of autonomous AI agents far easier than before. This specialization project report explores the feasibility of LLM-based GIS agents, investigating how ChatGPT is currently being used in the field of GIS, how such LLMs could be used if applied in larger systems, and how such systems can be implemented. This report seeks to highlight the strengths of current LLM-based technologies, but also their weaknesses, and suggest areas of improvements and possible solutions to overcome current limitations. These goals are achieved through a literature study and three experiments that aim to support the findings of the literature study. The literature study presents the body of work that has already been done in regard to the position of LLMs in the field of GIS, as well as planning strategies applied in LLM-based agents, and retrieval-augmented generation - that is, giving the LLM hooks into the real world. The three experiments focus on ChatGPT's ability to handle geospatial data in various formats and through different access channels. The overall goal of this specialisation project is to lay the groundwork for development of LLM-based GIS agents.},
  langid = {english},
  school = {NTNU}
}

@misc{hongMetaGPTMetaProgramming2023,
  title = {{{MetaGPT}}: {{Meta Programming}} for {{A Multi-Agent Collaborative Framework}}},
  shorttitle = {{{MetaGPT}}},
  author = {Hong, Sirui and Zhuge, Mingchen and Chen, Jonathan and Zheng, Xiawu and Cheng, Yuheng and Zhang, Ceyao and Wang, Jinlin and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin and Schmidhuber, J{\"u}rgen},
  year = {2023},
  month = nov,
  number = {arXiv:2308.00352},
  eprint = {2308.00352},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.00352},
  url = {http://arxiv.org/abs/2308.00352},
  urldate = {2024-03-21},
  abstract = {Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\8QSJWCNV\\Hong et al. - 2023 - MetaGPT Meta Programming for A Multi-Agent Collab.pdf;C\:\\Users\\oskar\\Zotero\\storage\\UWREAZPZ\\2308.html}
}

@misc{langchainaiLangchainaiLangchain2022,
  title = {Langchain-Ai/Langchain},
  author = {{LangChain AI}},
  year = {2022},
  month = oct,
  url = {https://github.com/langchain-ai/langchain},
  urldate = {2023-10-05},
  abstract = {⚡ Building applications with LLMs through composability ⚡},
  copyright = {MIT},
  howpublished = {LangChain AI}
}

@misc{langchainaiLangchainaiLanggraph2024,
  title = {Langchain-Ai/Langgraph},
  author = {{LangChain AI}},
  year = {2024},
  month = mar,
  url = {https://github.com/langchain-ai/langgraph},
  urldate = {2024-03-21},
  copyright = {MIT},
  howpublished = {LangChain AI}
}

@misc{longLargeLanguageModel2023,
  title = {Large {{Language Model Guided Tree-of-Thought}}},
  author = {Long, Jieyi},
  year = {2023},
  month = may,
  number = {arXiv:2305.08291},
  eprint = {2305.08291},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.08291},
  urldate = {2024-02-12},
  abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: {\textbackslash}url\{https://github.com/jieyilong/tree-of-thought-puzzle-solver\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\2RDF9TXE\\Long - 2023 - Large Language Model Guided Tree-of-Thought.pdf;C\:\\Users\\oskar\\Zotero\\storage\\TJG6C6J7\\2305.html}
}

@misc{maiOpportunitiesChallengesFoundation2023b,
  title = {On the {{Opportunities}} and {{Challenges}} of {{Foundation Models}} for {{Geospatial Artificial Intelligence}}},
  author = {Mai, Gengchen and Huang, Weiming and Sun, Jin and Song, Suhang and Mishra, Deepak and Liu, Ninghao and Gao, Song and Liu, Tianming and Cong, Gao and Hu, Yingjie and Cundy, Chris and Li, Ziyuan and Zhu, Rui and Lao, Ni},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06798},
  eprint = {2304.06798},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06798},
  url = {http://arxiv.org/abs/2304.06798},
  urldate = {2024-01-24},
  abstract = {Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial subdomains including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, these task-agnostic LLMs can outperform task-specific fully-supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image-based urban noise intensity classification, and remote sensing image scene classification), existing foundation models still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal foundation model which can reason over various types of geospatial data through geospatial alignments. We conclude this paper by discussing the unique risks and challenges to develop such a model for GeoAI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,I.2.0,I.2.10,I.2.4,I.2.7,I.5.1},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\X82XT37U\\Mai et al. - 2023 - On the Opportunities and Challenges of Foundation .pdf;C\:\\Users\\oskar\\Zotero\\storage\\JUDG2RZS\\2304.html}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2023-10-09},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\U4ISGUBN\\OpenAI - 2023 - GPT-4 Technical Report.pdf;C\:\\Users\\oskar\\Zotero\\storage\\ANYBWPS8\\2303.html}
}

@misc{openaiIntroducingChatGPT2022,
  title = {Introducing {{ChatGPT}}},
  author = {{OpenAI}},
  year = {2022},
  month = nov,
  url = {https://openai.com/blog/chatgpt},
  urldate = {2023-10-26},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  langid = {american},
  file = {C:\Users\oskar\Zotero\storage\BXCIMZNC\chatgpt.html}
}

@inproceedings{radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik},
  year = {2018},
  url = {https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035},
  urldate = {2023-10-09},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  file = {C:\Users\oskar\Zotero\storage\HR6GDGYQ\Radford and Narasimhan - 2018 - Improving Language Understanding by Generative Pre.pdf}
}

@misc{rajkumarEvaluatingTexttoSQLCapabilities2022,
  title = {Evaluating the {{Text-to-SQL Capabilities}} of {{Large Language Models}}},
  author = {Rajkumar, Nitarshan and Li, Raymond and Bahdanau, Dzmitry},
  year = {2022},
  month = mar,
  number = {arXiv:2204.00498},
  eprint = {2204.00498},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.00498},
  url = {http://arxiv.org/abs/2204.00498},
  urldate = {2024-02-08},
  abstract = {We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Machine Learning},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\E9NBMPWI\\Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf;C\:\\Users\\oskar\\Zotero\\storage\\HWQVPKL2\\2204.html}
}

@misc{salehinejadRecentAdvancesRecurrent2018,
  title = {Recent {{Advances}} in {{Recurrent Neural Networks}}},
  author = {Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
  year = {2018},
  month = feb,
  number = {arXiv:1801.01078},
  eprint = {1801.01078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01078},
  url = {http://arxiv.org/abs/1801.01078},
  urldate = {2024-03-21},
  abstract = {Recurrent neural networks (RNNs) are capable of learning features and long term dependencies from sequential and time-series data. The RNNs have a stack of non-linear units where at least one connection between units forms a directed cycle. A well-trained RNN can model any dynamical system; however, training RNNs is mostly plagued by issues in learning long-term dependencies. In this paper, we present a survey on RNNs and several new advances for newcomers and professionals in the field. The fundamentals and recent advances are explained and the research challenges are introduced.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\X82WQCGT\\Salehinejad et al. - 2018 - Recent Advances in Recurrent Neural Networks.pdf;C\:\\Users\\oskar\\Zotero\\storage\\JRLED8RL\\1801.html}
}

@misc{sanfilippoRedisRealtimeData2009,
  title = {Redis - {{The Real-time Data Platform}}},
  author = {Sanfilippo, Salvatore},
  year = {2009},
  journal = {Redis},
  url = {https://redis.io/},
  urldate = {2024-04-19},
  abstract = {Developers love Redis. Unlock the full potential of the Redis database with Redis Enterprise and start building blazing fast apps.},
  langid = {english},
  file = {C:\Users\oskar\Zotero\storage\ZJXVH295\redis.io.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/1706.03762v7},
  urldate = {2023-10-10},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  langid = {english},
  file = {C:\Users\oskar\Zotero\storage\KG8K58TW\Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@misc{wuAutoGenEnablingNextGen2023a,
  title = {{{AutoGen}}: {{Enabling Next-Gen LLM Applications}} via {{Multi-Agent Conversation}}},
  shorttitle = {{{AutoGen}}},
  author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
  year = {2023},
  month = oct,
  number = {arXiv:2308.08155},
  eprint = {2308.08155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.08155},
  url = {http://arxiv.org/abs/2308.08155},
  urldate = {2024-03-21},
  abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\IDKBMDML\\Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf;C\:\\Users\\oskar\\Zotero\\storage\\K6DAI6WM\\2308.html}
}
