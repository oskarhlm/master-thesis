\chapter{Related Work}
\label{cha:related_work}

\begin{comment}
What other research has been conducted in this area and how is it related to your work?
This section is thus where your literature review will be presented. It is important when presenting the review
that you give an overview of the motivating elements of the work going on in your field and how these relate to your work,
rather than a list of contributors and what they have done.
This means that you need to extract the key important factors for your work and discuss how others have addressed
each of these factors and what the advantages/disadvantages are with such approaches.
As you mention other authors, you should reference their work.
Note that the reference list reflects the literature you have read {\em and\/} have cited.
This will only be a subset of the literature that you have read.

A good way to find relevant work is by checking what others are referencing, e.g., in papers you have already found
or in previous studies carried out at NTNU, such as \cite{Berg;Gopinathan:17}.
However, when doing that,
do not fall into one of the common traps, such as re-iterating someone's false quote or faulty analysis of
a previous paper (check the original source!), or getting stuck inside a local research cluster (a group of
researchers that mainly refer to the ones using the same type of approaches or similar ideas).

Make sure that it is clear how and why you decided to include some references (and discard others). As in all parts of research, it should ideally be possible for someone else to reproduce your work, also when it comes to finding the relevant references.
There are (at least) three basic methods for finding references:
\begin{enumerate}
    \item Trust the authorities (e.g., your supervisor) to dig out good texts for you.
          Those can often be used as a seed set for:
    \item Snowballing, where you have some good articles and check the references in them for other good ones.
          Note that this can be done both backwards and forwards on the timeline; that is, using tools like Google Scholar, you can also check who refers \textit{to\/} the good articles you have already found.
    \item Carry out a Systematic Literature Review (or Structured Literature Review, SLR), a method introduced more formally into Software Engineering by \citet{Kitchenham;Charters:07}, but based on several similar methods for other disciplines.
          At the core of the method is an SQL-related  search over a reference database (such as Google Scholar).
          A good introduction to SLR is given by \citet{Kofod-Petersen:14}.
\end{enumerate}

Note that a reference needs to be complete: you should always give the full name of a conference or journal,
always include page numbers, always say where a book or thesis was published, and where a conference took place, as further described in Section~\ref{sec:reference_list}.

Just as described in the Background chapter (Chapter~\ref{cha:background_theory}), it is possible (and even likely) that you will want to reuse some of the text that you have written for your specialisation project in your Master's Thesis.
This is allowed, as long as it is clearly stated what you have reused and in what form (e.g., if a section is a straight-forward copy, if it has undergone only editorial changes, if it contains some old material but also some new, etc.).
\end{comment}

\begin{itshape}
    NB! Parts of the \nameref{cha:related_work} chapter is reused material from the specialization project \citep{holmLLMsDeathGIS2023} preceding this master thesis. Below are the sections in question, together with a description of the extent to which, and how, the material is reused:

    \begin{itemize}
        \item \Autosubsectionref{sec:llm-gis}: Reused without modification.
    \end{itemize}
\end{itshape}

\vspace{12pt}

This chapter provides an overview of previous research that shares objectives similar to those of this thesis. The related work is divided into two main parts: \autoref{sec:llm-gis}, which presents research investigating potential use cases of \acrshortpl{acr:llm} in the space of geospatial information technology; and \autoref{sec:agent-patterns}, which presents examples of three different types of patterns commonly used in \acrshort{acr:llm}-based agents.

\section[Using LLMs for Geospatial Purposes]{Using \acrshortpl{acr:llm} for Geospatial Purposes}
\label{sec:llm-gis}

\cite{robertsGPT4GEOHowLanguage2023} investigated the extent of GPT-4's geospatial awareness through a set of case studies with increasing difficulty, starting with general factual tasks and finishing with complex questions such as generating country outlines and travel networks. The authors found that \acrshort{acr:gpt}-4 is \enquote{skillful at solving a variety of application-centric tasks}, almost having the ability to \enquote{see}, despite being a language model and therefore only able to interface with the world through sequenced, textual input. Examples include its ability to serve as a travel assistant in providing itinerary suggestions for a trip when provided with requirements, and its ability to provide generally correct start and end locations of bird migration paths. While it quickly became obvious that a lot of geospatial context have been embedded within the model during the vast pre-training, the question of whether this is memorization or reasoning is a central one. The authors suggest that the variability of tasks in their experiments deems it unlikely that it is all memorization, but they say that some things appear to be memorized.

\cite{mooneyUnderstandingGeospatialSkills2023} examined the performance of ChatGPT in a \acrfull{acr:gis} exam, aiming to assess its ability to grasp various geospatial concepts, highlighting its capabilities and limitations. Experiments were conducted on GPT-3.5 and GPT-4, which delivered performances equivalent to grades of D and B+, respectively. Additional experiments were conducted for more specialized areas of \acrshort{acr:gis}, including True/False questions about spatial analysis, and simple tasks in applied \acrshort{acr:gis} workflows. Experiments on the latter showed that ChatGPT-4 was able to correctly answer a relatively complex \acrshort{acr:gis} task involving seven different datasets, requiring seven steps in order to obtain a perfect score. Generally, ChatGPT-4 outperformed ChatGPT-3.5 in all tasks. While clearly powerful, the authors highlight a range of limitations, among which the multimodal nature of \acrshort{acr:gis}, which would hinder a straightforward application of existing models.

\cite{liAutonomousGISNextgeneration2023} state that \enquote{autonomous \acrshort{acr:gis} will need to achieve five autonomous goals: self-generating, self-organizing, self-verifying, self-executing, and self-growing.}. They provide a \enquote{divide-and-conquer}-based method to address some of these goals. Furthermore, they propose a simple trial-and-error approach to address the self-verifying goal. They also highlight the need for a memory system in a mature \acrshort{acr:llm}-based \acrshort{acr:gis} system, referring to the use of vector databases in autonomous agents made with AutoGPT \citep{richardAutoGPTHeartOpensource2023}. Even with its shortages, the solution that \cite{liAutonomousGISNextgeneration2023} provide---called \acrshort{acr:llm}-Geo---is able to produce good solutions in various case studies by providing executable assemblies in a Python environment when provided with URLs to relevant data sets, along with a user-specified query.

\cite{zhangGeoGPTUnderstandingProcessing2023} use the LangChain framework in order to combine different GIS tools in a sequence to solve various sub-goals, focusing on using the semantic understanding and reasoning abilities of \acrshortpl{acr:llm} to call externally defined tools, employing the \acrshort{acr:llm} as an agent or controller. The authors take great inspiration from the AutoGPT framework \citep{richardAutoGPTHeartOpensource2023}. The externally defined tools are described (manually) by their names and descriptions. These descriptions contain information about the input parameters and output types of the tools/functions. Tools are defined for geospatial data collection, data processing and analysis, and data visualization. The effectiveness of the system is showcased in four case studies.


\section{Agent Patterns}
\label{sec:agent-patterns}

\acrshort{acr:llm}-based agents can be implemented in many ways, and researchers have developed a plethora of \textit{agent patterns} that seek to improve upon areas where \acrshortpl{acr:llm} tend to be less effective. This includes patterns for retrieval of external data, as well as multi-agent patterns. Such patterns can improve the performance of \acrshort{acr:llm}-based agents within any domain, and should also be considered when developing one for geospatial purposes. The following sections will shortly explain some of the patterns that were considered in the development of GeoGPT.

The \textbf{multi-agent} pattern that takes inspiration from human collaboration in that it is made up from multiple specialized agents that work together to achieve some objective. There have been several implementations of the pattern, with certain differences. MetaGPT \citep{hongMetaGPTMetaProgramming2023} is a \acrshort{acr:llm}-based multi-agent system consisting of agents with human-level domain expertise. Using an assembly line paradigm, where the overall goal is divided into subtasks, \citeauthor{hongMetaGPTMetaProgramming2023} showed that MetaGPT could generate more coherent solutions compared to the previous state-of-the-art multi-agent systems. At the time of release, MetaGPT set a new state-of-the-art performance on the HumanEval and \acrshort{acr:mbpp} benchmarks \citep[7]{hongMetaGPTMetaProgramming2023}, demonstrating the potential of the multi-agent pattern.

Patterns that employ \textbf{self-reflection} are commonly used with autonomous \acrshort{acr:llm}-based agents. \textit{Reflextion} \citep{shinnReflexionLanguageAgents2023} is a pattern/framework that reinforces agents through linguistic feedback, essentially allow the agent to reflect upon the consequence of its decisions. The framework utilizes three distinct models: an \textit{Actor} model responsible for generating text and actions; an \textit{Evaluator} model which assess the quality of the outputs from the Actor; and a \textit{Self-Reflection} model that generates reinforcement cues for the Actor based on the output and quality assessment of the other two models. Together, these three models form a loop that will run until the Evaluator deems the output from the Actor as correct.

\textbf{Step-by-step reasoning} is another pattern that has proved to be efficient in helping \acrshortpl{acr:llm} produce correct responses. \cite{weiChainofThoughtPromptingElicits2023a} demonstrated that so-called \textit{chain-of-thought prompting} can be used for enhancing reasoning in \acrshortpl{acr:llm} by providing it with examples of how to reason for \textit{similar} to that which it is trying to solve. By helping the \acrshort{acr:llm} with decomposing multi-step problems into intermediate steps in this way, \citeauthor{weiChainofThoughtPromptingElicits2023a} managed to achieve state-of-the-art accuracy on the GSM8K benchmark of math word problems.


\glsresetall