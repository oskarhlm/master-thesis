\chapter{Discussion}
\label{cha:discussion}

% \section{Evaluation}
% \label{sec:evaluation}
\begin{comment}

When evaluating your results, avoid drawing grand conclusions, beyond those that your results can in fact support.
Further, although you may have designed your experiments to answer certain questions,
the results may raise other questions in the eyes of the reader.
It is important that you study the graphs/tables to look for unusual features/entries, and discuss these as well as the main findings.
In particular, carry out an error analysis: What went wrong and why?

A confusion matrix can, for example, be a good way to display misclassifications.
Figure~\ref{fig:conf_sentiment} (on Page~\pageref{fig:conf_sentiment}) shows two confusion matrices.
If there were perfect correlation between true and predicted labels, the long diagonals (from the upper left to the lower right corner) would be completely red.
However,  the confusion matrices indicate
that this classifier was quite biased towards the neutral label (illustrated with \Neutrey),
as can be seen from the warm colours in the positive (\Smiley) and negative (\Sadey) true label cells of the \Neutrey predicted label column.

% Axis configuration for confusion matrices with pgfplots
\pgfplotsset{
    colormap={whitehot}{color(0cm)=(white); color(1cm)=(yellow); color(2cm)=(orange); color(3cm)=(red)},
    confusionaxis/.style={
            colorbar,
            colorbar style={
                    width=2mm,
                    at={(1.05,1)},
                },
            colormap name=whitehot,
            faceted color=none, % remove lines between fields
            view={0}{90},
            y dir=reverse,
            xlabel=Predicted label,
            ylabel=True label,
            tick style={draw=none},
            yticklabels={,,},
            xticklabels={,,},
            every node=[font=\small],
            extra x ticks={0.4,1.5,2.6},
            extra x tick labels={\Smiley, \Neutrey, \Sadey},
            extra y ticks={0.3,1.5,2.7},
            extra y tick labels={\Smiley, \Neutrey, \Sadey},
            extra x tick style={
                    x tick label style={
                            font=\Large
                        }
                },
            extra y tick style={
                    y tick label style={
                            font=\Large
                        }
                },
            width=.4\linewidth,
        }
}

\begin{figure}[t!]
    \centering
    \begin{subfigure}{\linewidth}
        \begin{tikzpicture}
            \begin{axis}[
                    confusionaxis,
                    title={\em Without normalization},
                ]
                \addplot3
                [surf,mesh/cols=4,shader=flat corner
                ] coordinates {
                        (0,0,740) (1,0,490 ) (2,0,43 ) (3,0,1)
                        (0,1,102) (1,1,1229) (2,1,38 ) (3,1,1)
                        (0,2,28 ) (1,2,240 ) (2,2,199) (3,2,1)
                        (0,3,1  ) (1,3,1   ) (2,3,1  ) (3,3,1)
                    };
            \end{axis}
        \end{tikzpicture}
        %\hfill
        \begin{tikzpicture}
            \begin{axis}[
                    confusionaxis,
                    title={\em With normalization},
                    ylabel={},
                    colorbar style={
                            ylabel={},
                            yticklabel style={
                                    align=right,
                                }
                        },
                ]
                \addplot3
                [surf,mesh/cols=4,shader=flat corner
                ] coordinates {
                        (0,0,0.58130401) (1,0,0.38491752) (2,0,0.03377848) (3,0,1)
                        (0,1,0.07450694) (1,1,0.89773557) (2,1,0.02775749) (3,1,1)
                        (0,2,0.05995717) (1,2,0.51391863) (2,2,0.4261242 ) (3,2,1)
                        (0,3,1         ) (1,3,1         ) (2,3,1         ) (3,3,1)
                    };
            \end{axis}
        \end{tikzpicture}
        \label{fig:conf_sentiment_2013}
    \end{subfigure}
    \caption{Sentiment classifier confusion matrices}
    \label{fig:conf_sentiment}
\end{figure}

\end{comment}
% \section{Discussion}
% \label{sec:discussion}
\begin{comment}

In this section it is important to include a discussion of not just the merits of the work conducted, but also the limitations.
Which choices did you make? Why? What alternatives were there?
{\color{red}\textbf{Note that a key part of the Master's Thesis grading is based on the student's ability to discuss the results in light of the work by others as well as the restrictions and potential of the work itself.}}
While the Results section will report the outcome of each specific experiments, the Discussion should put those results into perspective and look at overall lessons that can be learned from the entire series of experiments.

You should be able to discuss your work in relation to its overall goal and your research questions (i.e., those introduced in Chapter~\ref{cha:introduction}),
but also address issues such as any ethical considerations that the work may entail,
as well as its technical challenges and limitations.

Discussion and evaluation can either be two different chapters, a joint chapter (as here), or part of the concluding chapter
--- or the discussion can be part of that chapter while the evaluation is part of the experimental chapter.

As for most parts of the thesis, it is possible to select various outlines and setups for the discussion; the important thing is that all the relevant parts appear \textit{somewhere\/} in the text.
\end{comment}

\section[GeoGPT's Place in the Field of GIS]{GeoGPT's Place in the Field of \acrshort{acr:gis}}

As stated in the \nameref{sec:goals-and-research-questions} section, one of the goals of this thesis was to see if \acrshort{acr:llm}-based systems can replace \acrshort{acr:gis} professionals. From the results of the experiments conducted it seems that this is not the case --- as of yet. Results from experiments comparing prompts designed to emulate expert and novice users, as detailed in \autoref{subsec:prompt-quality-test-results}, indicate that the role of \acrshort{acr:gis} professionals is not immediately threatened. As \autoref{fig:novice-vs-expert-munkegata-trees} shows, expert-level prompting is highly necessary to make GeoGPT produce the desired outcome for more complex tasks.

It is, however, clear that \acrshort{acr:llm} technologies have significant potential to automate tasks that \acrshort{acr:gis} professionals are commonly faced with. GeoGPT can therefore serve as a helpful companion --- much like GitHub Copilot\footnote{\url{https://github.com/features/copilot}} for software developers --- that can quickly solve a simple, repetitive tasks, alleviating the workload on the \acrshort{acr:gis} professional. The benchmarking results (see \autoref{tbl:test-results-quantitative}) show that a system like GeoGPT is able to successfully solve a wide range of geospatial task. Note also that many of the questions in the benchmarking dataset are not very technically phrased, showing that expert-level prompting is not necessary for simpler tasks.

At the same time, the results show that the indeterminate nature of \acrshortpl{acr:llm} gives reason to doubt the answers that GeoGPT produces. \autoref{tbl:stddev-by-agent-type} shows GeoGPT's answers are not always consistent. A common problem with many \acrshortpl{acr:llm} is that they will often deliver overly answers in response to questions they do not know the answer to, and these numbers show that GeoGPT suffers from the same issues. This leads to issues, as a user with limited \acrshort{acr:gis} experience will generally not be capable of detecting when GeoGPT generates a believable but completely false answer. An example of this was seen in the benchmarking tests (see \autoref{tbl:test-results-quantitative}) when asking which (Norwegian) county is the largest by size. This question was asked a total of nine times across different agents and was answered incorrectly four times (see \autoref{app:test-results}). These incorrect answers typically stated the following:

\begin{quote}
    The largest county by size is **Finnmark**, with an area of approximately **646,150 square kilometers**.
\end{quote}

The correct answer to the question --- based on the data available to GeoGPT --- is \enquote{Innlandet}, which area is calculated to about 80,5 thousand square kilometres. To the inexperienced user there is no way of knowing whether this answer can be trusted. An experienced user could, however, inspect the code produced by GeoGPT to see if it makes sense. A \acrshort{acr:gis} professional should also be able to recognise that 646,150 square kilometres is far greater than the actual size of Finnmark.

\citep[1-2]{linGeneratingConfidenceUncertainty2023} write that the issue with uncertainty in \acrshortpl{acr:llm} is a challenge that \enquote{has attracted limited attention until recently}, highlighting the \enquote{forbiddingly high} dimensionality of the output space as one of the key hindrances to a reliable way of measuring confidence. They also mention the fact that many \acrshortpl{acr:llm} are closed-source (like OpenAI's \acrshort{acr:gpt} models and Anthropic's Claude models) and served via \acrshortpl{acr:api} as black-boxes.

\section[Limitations with OGC API Features]{Limitations with \acrshort{acr:ogc} \acrshort{acr:api} Features}
\label{sec:difficulties-with-oaf}

A common source of error with several of the tests conducted with the \acrshort{acr:ogc} \acrshort{acr:api} Features agent in was its inability to fetch more than 10,000 features from the server. The limit of 10,000 features is specified in the Features standard \citep{opengeospatialconsortiumOGCAPIFeatures2022}, meaning that no more than 10,000 features should be returned in a single response. Accompanied by such a large response, however, should be a \texttt{next} link than should point to the next set of 10,000 features. This way, the server could return more than 10,000 features. Unfortunately, as of \today, the current version of \textit{pg\_featuresserv} does not support this feature\footnote{\url{https://github.com/CrunchyData/pg_featureserv/blob/master/FEATURES.md}}, which is a significant limitation to the current \acrshort{acr:ogc} \acrshort{acr:api} Features implementation for GeoGPT.

Furthermore, the lack of multi-collection queries is in the author's opinion a big limitation to the current Features specification. A proposal draft\footnote{\url{https://github.com/opengeospatial/ogcapi-features/tree/master/proposals/search}} to such features have been created, but it is unclear whether this will accepted into the specification. This extension, called \textit{Search}, would allow for more complex \acrshort{acr:cql} queries than are not easily specified using query parameters. \autoref{code:multi-collection-cql} shows one of the multi-collection query examples included in the proposal draft. Being able to construct such queries could make retrieval of features much more efficient, possibly making the Python tool in the current GeoGPT implementation redundant. This query would not be possible using the current implementation, and one would have to download the two collections, load them into memory using Python, and perform the \texttt{contains} operation there. Being able to construct such \acrshort{acr:cql} queries would offload this work to the Features server which (if the data source is a PostGIS database) would run very efficient \acrshort{acr:sql} code instead of the less than optimal Python code that would otherwise be necessary.

\begin{lstlisting}[
    caption=Multi-collection CQL query using the \textit{Search} extension,
    label=code:multi-collection-cql
]
\\ SQL query for fetching lakes within Algonquin Park
SELECT lakes.*
FROM lakes
JOIN parks ON ST_Intersects(lakes.geometry, parks.geometry)
WHERE parks.name = 'Algonquin Park';

\\ Corresponding CQL query (would return a tuple of parks and lakes)
POST /search   HTTP/1.1                                           
Host: www.someserver.com/                                         
Accept: application/json                                          
Content-Type: application/ogcqry+json                             
                                                                    
[                                                                 
    {                                                              
        "collections": ["parks","lakes"]                            
        "filter": {                                                 
            "and": [                                                 
            {"eq": [{"property": "parks.name"},"Algonquin Park"]} 
            {"contains": [{"property": "parks.geometry"},         
                            {"property": "lakes.geometry"}]}        
            ]                                                        
        }                                                           
    }                                                              
]
\end{lstlisting}


\section{Self-Verification}

GeoGPT is already doing self-verfication through the system messages that verify that a file has been saved to the working directory and that a layer has been added to the map on the client. This does not, however, fix the issue where GeoGPT works with different data to what it believes it to be. An example of this edge case was presented in the section on unsuccessful test runs in \autoref{subsec:quantitative-results} where GeoGPT believed it was working with data only for Oslo, when in reality the data was for the whole of Norway.

A possible way of doing self-verification that mitigates these kinds of issues is to utilize multi-modal \acrshortpl{acr:llm}. \autoref{fig:chatgpt-visual-self-verfication} shows how using a multi-modal \acrshort{acr:gpt}-4 model can correctly identify that the resulting layer from the above-mentioned unsuccessful analysis is incorrect, based only on a screenshot of the map.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{oslo_roads_maxspeed_hte_70_kmh_python_clipped_chat_response.png}
    \caption{ChatGPT multi-modal \acrshort{acr:gpt}-4 correctly identifying that the map layer supposed to show high-speed roads in Oslo in fact \enquote{extends far beyond Oslo}}
    \label{fig:chatgpt-visual-self-verfication}
\end{figure}

\FloatBarrier

\section{Multi-Agent Architectures}
\label{sec:multi-agent-architectures}

An attempt was made to create a multi-agent version of GeoGPT that employs three different sub-agents: one for data retrieval, one for analysis, and for map interaction. These agents are orchestrated by a \textit{Supervisor} agent, which takes input from the user and assigns tasks to the appropriate sub-agents. \autoref{fig:agent-supervisor} shows how a supervisor node takes a user message (or the chat history up to that point) and selects which sub-agent is to solve the next sub-task. Each agent fill have their own set of tools, that is, functions/tools that they have available at their disposal through \textit{function calling} (see \autoref{subsec:function-calling}). Inspired by Microsoft's \textit{AutoGen} framework\footnote{\url{https://microsoft.github.io/autogen/}} --- which provides a high-level abstraction for developing multi-agent conversations --- and MetaGPT (further discussed in \autoref{sec:agent-patterns}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{agent_supervisor.png}
    \caption{Illustration of how an agent supervisor takes in a user message and selects which sub-agent is to solve the next sub-task}
    \label{fig:agent-supervisor}
\end{figure}

Initial tests on a multi-agent version of the \acrshort{acr:ogc} \acrshort{acr:api} Features agent were conducted, but it turned out to both increase the latency of the system and be a source of confusion for the \acrshort{acr:llm}. It is possible that such an architecture could be useful as the number of functions/tools available to the agnet grows --- seeing as a large number of functions tend to start confusing the \acrshort{acr:llm} --- but for the current version of GeoGPT the multi-agent pattern seemed to only get in the way.
